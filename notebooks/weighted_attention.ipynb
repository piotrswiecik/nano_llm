{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from nanollm.token import BPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[818, 4112, 640, 644, 2925, 2835, 2058, 1088, 13]\n",
      "# tokens = 9\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In absolute time what goes round comes around.\"\n",
    "\n",
    "bpe = BPETokenizer()\n",
    "tokens = bpe.tokenize(sentence)\n",
    "\n",
    "print(tokens)\n",
    "print(f\"# tokens = {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created embedding layer: Embedding(9, 3)\n",
      "tensor([[-1.1068,  1.0614, -0.1729],\n",
      "        [ 1.0466, -0.7614,  0.7629],\n",
      "        [-0.4582,  0.4299,  0.6685],\n",
      "        [-0.0931,  0.9515,  0.3471],\n",
      "        [ 0.8221, -0.9725, -0.6850],\n",
      "        [ 0.8352, -0.0795, -1.3230],\n",
      "        [-0.6154, -1.5507,  0.0692],\n",
      "        [ 0.0125, -0.6393,  0.5143],\n",
      "        [ 0.3528,  0.1270,  0.3728]])\n"
     ]
    }
   ],
   "source": [
    "# this simplified example assumes that the vocabulary is the set of tokens\n",
    "# (vocabulary has only 9 possible tokens)\n",
    "# each token is embedded into 3 dimensions\n",
    "# embeddings are initially randomized\n",
    "\n",
    "embedding = torch.nn.Embedding(len(tokens), 3)\n",
    "print(f\"created embedding layer: {embedding}\")\n",
    "print(embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tokens to initial embeddings\n",
    "\n",
    "inputs = embedding.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context vector example\n",
    "\n",
    "Calculating context vector for single token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token embedding x_2 at index 1: tensor([ 1.0466, -0.7614,  0.7629])\n",
      "dimensionality of input embeddings: 3\n",
      "dimensionality of context vectors: 3\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "print(f\"token embedding x_2 at index 1: {x_2}\")\n",
    "\n",
    "# this is the dimensionality of the input embeddings determined by the embedding layer\n",
    "d_in = inputs.shape[1]\n",
    "print(f\"dimensionality of input embeddings: {d_in}\")\n",
    "\n",
    "# this is the dimensionality of context vectors - a constant\n",
    "# typically this should be the same as the dimensionality of the input embeddings d_in\n",
    "# here we want it to be different to illustrate the concept\n",
    "d_out = 2\n",
    "print(f\"dimensionality of context vectors: {d_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learnable matrices used by self-attention are randomly initialized\n",
    "\n",
    "torch.manual_seed(0)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_2: tensor([0.6865, 1.1872])\n",
      "key_2: tensor([0.4322, 0.7633])\n",
      "value_2: tensor([0.3319, 0.3923])\n",
      "calculated query_2: torch.Size([2])\n",
      "calculated keys: torch.Size([9, 2])\n",
      "calculated values: torch.Size([9, 2])\n",
      "calculated attn scores for x_2: torch.Size([9])\n",
      "attn scores for x_2: tensor([-0.5454,  1.2029,  0.2943,  1.1301, -0.3735,  0.1375, -2.4616, -0.2940,\n",
      "         0.8964])\n",
      "calculated attn weights for x_2: torch.Size([9])\n",
      "attn weights for x_2: tensor([0.0606, 0.2087, 0.1098, 0.1982, 0.0684, 0.0982, 0.0156, 0.0724, 0.1680])\n",
      "calculated context vector for x_2: torch.Size([2])\n",
      "context vector for x_2: tensor([0.1613, 0.2270])\n"
     ]
    }
   ],
   "source": [
    "# calculate context vector c_2 for token x_2\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(f\"query_2: {query_2}\")\n",
    "print(f\"key_2: {key_2}\")\n",
    "print(f\"value_2: {value_2}\")\n",
    "\n",
    "# we will need key and value vectors for all tokens\n",
    "keys = inputs @ W_key # (9,3) @ (3,2) = (9,2)\n",
    "values = inputs @ W_value # (9,3) @ (3,2) = (9,2)\n",
    "print(f\"calculated query_2: {query_2.shape}\")\n",
    "print(f\"calculated keys: {keys.shape}\")\n",
    "print(f\"calculated values: {values.shape}\")\n",
    "\n",
    "# calculate attention scores for token x_2\n",
    "attn_scores_2 = query_2 @ keys.T # (2,) @ (9,2).T = (9,)\n",
    "print(f\"calculated attn scores for x_2: {attn_scores_2.shape}\")\n",
    "print(f\"attn scores for x_2: {attn_scores_2}\")\n",
    "\n",
    "# use softmax to convert attn scores into weights\n",
    "d_k = keys.shape[-1] # output dimensionality of key vectors\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(f\"calculated attn weights for x_2: {attn_weights_2.shape}\")\n",
    "print(f\"attn weights for x_2: {attn_weights_2}\")\n",
    "assert torch.allclose(attn_weights_2.sum(), torch.tensor(1.0)) # softmax sums up to 1\n",
    "\n",
    "# calculate context vector c_2\n",
    "context_2 = attn_weights_2 @ values # attn weights used to weigh values of all tokens surrounding x_2\n",
    "print(f\"calculated context vector for x_2: {context_2.shape}\") # (9,) @ (9,2) = (2,)\n",
    "print(f\"context vector for x_2: {context_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1516, -0.2833],\n",
       "        [ 0.1613,  0.2270],\n",
       "        [ 0.0130,  0.0065],\n",
       "        [ 0.0400,  0.0489],\n",
       "        [ 0.0165,  0.0106],\n",
       "        [-0.0241, -0.0569],\n",
       "        [-0.1195, -0.2228],\n",
       "        [ 0.0342,  0.0401],\n",
       "        [ 0.0776,  0.1064]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention via dedicated layer\n",
    "\n",
    "from nanollm.attn import SelfAttention\n",
    "\n",
    "torch.manual_seed(0)\n",
    "attn = SelfAttention(3, 2)\n",
    "\n",
    "out = attn(inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
