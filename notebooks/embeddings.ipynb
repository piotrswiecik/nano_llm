{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-independent embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<pad>\", \"<unk>\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "embedding = torch.nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4])\n",
      "Parameter containing:\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339],\n",
      "        [ 0.8487,  0.6920, -0.3160, -2.1152],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [ 0.1198,  1.2377,  1.1168, -0.2473],\n",
      "        [-1.3527, -1.6959,  0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530],\n",
      "        [-0.2159, -0.7425,  0.5627,  0.2596],\n",
      "        [-0.1740, -0.6787,  0.9383,  0.4889],\n",
      "        [ 1.2032,  0.0845, -1.2001, -0.0048],\n",
      "        [-0.5181, -0.3067, -1.5810,  1.7066]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# initialize the embedding layer with random weights\n",
    "# each word is represented by dim=4 tensor -> 10 words -> 10x4 tensor\n",
    "\n",
    "print(embedding.weight.shape)\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 2])\n",
      "tensor([[ 0.3223, -1.2633,  0.3500,  0.3081],\n",
      "        [-1.3527, -1.6959,  0.5667,  0.7935],\n",
      "        [ 0.3223, -1.2633,  0.3500,  0.3081]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# in this type of embedding, both \"a\" tokens will have the same representation\n",
    "\n",
    "sequence = [\"a\", \"c\", \"a\"]\n",
    "sequence_indices = torch.tensor([vocab.index(token) for token in sequence])\n",
    "print(sequence_indices)\n",
    "print(embedding(sequence_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-aware embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length (# of characters): 20479\n",
      "corpus length (estimated # of words): 3634\n"
     ]
    }
   ],
   "source": [
    "with open(\"../assets/corpus_01.txt\", \"r\") as file:\n",
    "    corpus = file.read()\n",
    "\n",
    "print(f\"corpus length (# of characters): {len(corpus)}\")\n",
    "print(f\"corpus length (estimated # of words): {len(corpus.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt tokenizer vocab length: 50257\n"
     ]
    }
   ],
   "source": [
    "from nanollm.token import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "\n",
    "# gpt-2 tokenizer has a vocabulary size of 50257 - this is predetermined\n",
    "print(f\"gpt-2 tokenizer vocab length: {tokenizer._engine.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanollm.data import create_dataloader\n",
    "\n",
    "loader = create_dataloader(\n",
    "    corpus, batch_size=4, max_length=4, stride=4, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer._engine.n_vocab\n",
    "embedding_dim = 256\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original corpus length (# chars): 20479\n",
      "original corpus length (est # words): 3634\n",
      "tokenized corpus length: 5136\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = [\n",
    "    token.item() for batch in loader for sample in batch[0] for token in sample\n",
    "]\n",
    "\n",
    "# tokenized corpus length is < original corpus length in chars \n",
    "# but longer then original corpus length in words\n",
    "# this is typical for bpe tokenization\n",
    "print(f\"original corpus length (# chars): {len(corpus)}\")\n",
    "print(f\"original corpus length (est # words): {len(corpus.split())}\")\n",
    "print(f\"tokenized corpus length: {len(tokenized_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# occurences of word I = 29\n",
      "first 5 occurences = [0, 379, 736, 882, 895]\n"
     ]
    }
   ],
   "source": [
    "# find all occurences of token 40 that represents word \"I\"\n",
    "\n",
    "indices = [i for i, token in enumerate(tokenized_corpus) if token == 40]\n",
    "print(f\"# occurences of word I = {len(indices)}\")\n",
    "print(f\"first 5 occurences = {indices[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "# produce an embedding without any context information\n",
    "# the token representing word \"I\" becomes a 256-dimensional column tensor\n",
    "\n",
    "print(embedding_layer(torch.tensor([40])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of embedded corpus: torch.Size([5136, 256])\n"
     ]
    }
   ],
   "source": [
    "# convert the entire tokenized corpus to 256-dimensional embeddings\n",
    "\n",
    "corpus_as_tensor = torch.cat([sample for batch in loader for sample in batch[0]])\n",
    "embedding_0 = embedding_layer(corpus_as_tensor) # position-agnostic embeddings\n",
    "print(f\"size of embedded corpus: {embedding_0.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token #0 = 40, token #379 = 40\n",
      "embedding for token #0 = tensor([-0.1018, -0.6005, -0.6997,  0.4333, -0.4055], grad_fn=<SliceBackward0>)\n",
      "embedding for token #379 = tensor([-0.1018, -0.6005, -0.6997,  0.4333, -0.4055], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compare embeddings for two different occurences of word \"I\"\n",
    "\n",
    "print(f\"token #0 = {corpus_as_tensor[0]}, token #{indices[1]} = {corpus_as_tensor[indices[1]]}\")\n",
    "\n",
    "# the embeddings are exactly the same - no positional information is encoded\n",
    "print(f\"embedding for token #0 = {embedding_0[0][:5]}\")\n",
    "print(f\"embedding for token #{indices[1]} = {embedding_0[indices[1]][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a layer for absolute positional embedding\n",
    "# the underlying logic is that we define a context window length (e.g. 4)\n",
    "# and we assign an embedding value (256-dims) to each position in the context window\n",
    "\n",
    "# this will produces a 4x256 matrix\n",
    "\n",
    "context_length = 4\n",
    "positional_embedding_layer = torch.nn.Embedding(context_length, embedding_dim)\n",
    "\n",
    "# this creates the (4, 256) embedding matrix\n",
    "positional_embeddings = positional_embedding_layer(torch.arange(context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4794, -0.7480, -0.0834,  ...,  0.8273,  1.7678, -0.8788],\n",
       "        [-0.4280, -1.7475, -0.7723,  ..., -3.2978, -0.1759,  1.7628],\n",
       "        [ 0.7874,  1.3287,  1.1307,  ..., -0.4192,  0.7647, -0.3317],\n",
       "        [ 0.7321, -1.6347, -0.0794,  ...,  1.1676, -0.0695, -1.4688]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a sample tokenized sequence: tensor([  40,  367, 2885, 1464])\n",
      "base embedding: tensor([[-0.1018, -0.6005, -0.6997,  ..., -0.1395, -2.3091,  1.4107],\n",
      "        [-0.2515, -0.8558,  1.0292,  ...,  1.3501, -0.3530, -0.4898],\n",
      "        [-0.5669,  0.4898, -0.6491,  ..., -1.3058,  1.2083, -1.6056],\n",
      "        [-1.2992,  0.7592,  0.5555,  ...,  1.4055,  0.9644, -0.5949]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "positional embedding: tensor([[-0.5812, -1.3485, -0.7831,  ...,  0.6878, -0.5414,  0.5319],\n",
      "        [-0.6795, -2.6033,  0.2569,  ..., -1.9477, -0.5289,  1.2730],\n",
      "        [ 0.2206,  1.8185,  0.4815,  ..., -1.7250,  1.9730, -1.9373],\n",
      "        [-0.5671, -0.8755,  0.4761,  ...,  2.5731,  0.8949, -2.0637]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sample = loader.dataset[0][0]\n",
    "\n",
    "print(f\"a sample tokenized sequence: {sample}\")\n",
    "\n",
    "# to create positional embeddings, just add the matrices\n",
    "\n",
    "base_embedding = embedding_layer(sample)\n",
    "positional_embedding = positional_embedding_layer(torch.arange(context_length))\n",
    "sample_embedding = base_embedding + positional_embedding\n",
    "print(f\"base embedding: {base_embedding}\")\n",
    "print(f\"positional embedding: {sample_embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
